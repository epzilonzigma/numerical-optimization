{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cdac2b7-4bb7-4b93-8f98-d29fedb7a3ab",
   "metadata": {},
   "source": [
    "# Unconstrained Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d99a81-ef80-46a4-b9e9-76788d6b1413",
   "metadata": {},
   "source": [
    "This notebook with explore different algorithms for unconstrained optimization, specifically those which gradient (1st derivative) and second derivative information are available. It will open with a simple introduction of line search which extends to various related algorithms and then trust region methods. Additional algorithms maybe appended afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d8cdd5-4ef2-4fad-bc76-6a456d8b393b",
   "metadata": {},
   "source": [
    "To compute 1st and 2nd order derivatives, **autograd** package will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dc9ee93-364e-45a1-831c-995aad2ea675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "from scipy.optimize import minimize #for solving trust region subproblem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e11b039-ca85-41eb-bfd1-f1b87a84b1d9",
   "metadata": {},
   "source": [
    "## Line Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f5adc1-e9ce-4d22-bcce-bf79c950930e",
   "metadata": {},
   "source": [
    "### Naive Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4bb7d9-9b78-4eab-ac98-629af0cb24cb",
   "metadata": {},
   "source": [
    "Line search are a type of algorithms where given its initial point, it will seek \"along a line\" to find the local minima of a given objective function. \n",
    "\n",
    "There are a few things to consider when using line search algorithms:\n",
    "\n",
    "1. Initialization point\n",
    "2. Step size\n",
    "3. Direction\n",
    "4. Terminal condition (when to stop)\n",
    "\n",
    "Let's create a naive line search algorithm to get the basic intuition with the preceeding items address as the following:\n",
    "\n",
    "1. Initialization point - algo input\n",
    "2. Step size - algo inpu\n",
    "3. Direction - computed via gradient of the objective function\n",
    "4. Terminal condition - algo input as number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d351db7a-f1a8-47d0-8281-30f6a643f3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_line_search(objective_function, initial_point, step_size, iterations):\n",
    "    '''\n",
    "    returns arguments which identifies local minima of given objective function given \n",
    "        initial points, \n",
    "        step size, \n",
    "        and iterations\n",
    "    '''\n",
    "    \n",
    "    x = np.array([float(i) for i in initial_point])\n",
    "    i = 0\n",
    "    \n",
    "    while i < iterations:\n",
    "        \n",
    "        gradient = np.array(grad(objective_function)(x))*-1 # descent direction determined by negative gradient\n",
    "        unit_direction = gradient/np.linalg.norm(gradient) # desecnt direction factor effect minized to unit vector\n",
    "        x = list(x + unit_direction*step_size) # x is updated with x + step_size*direction\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09788c98-814f-4871-ad59-1cf4938070d7",
   "metadata": {},
   "source": [
    "With the above naive function, lets try the algorithm on f(x,y) = x^2 + y^2 where the minimum is at (0,0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da365fe8-14cb-429b-a6ee-78d2d439dec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0003040046208548086, 0.0001520023104274043]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def obj_fn(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "naive_line_search(obj_fn, [10, 5], 0.01, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c2e77b-be7e-46dc-b4db-478a59953915",
   "metadata": {},
   "source": [
    "We see that this is subsequently really close, lets see what happens if we were to change some parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "277f14af-3968-4078-b307-34eb4e85e5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25.103139891794168, 33.92316201594099]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_line_search(obj_fn, [37, 50], 0.01, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "518fb3b2-0735-4f7b-81e1-96adb0445d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.11973366456175838, 0.16180224940778176]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_line_search(obj_fn, [37, 50], 1, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "608e804b-d474-4255-925e-7f376face567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0699523462588483, -1.4458815489984427]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_line_search(obj_fn, [37, 50], 2, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b9e5a2d-022f-4d5c-acbb-beb56abc35de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.11973366456175838, 0.16180224940778176]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_line_search(obj_fn, [37, 50], 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad52b51c-f82d-4496-834d-7be2f45d1e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0699523462588483, -1.4458815489984427]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_line_search(obj_fn, [37, 50], 2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a503156-ed03-4f0e-8477-222381ecee48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[356.4191603676318, 385.3180112082782]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_line_search(obj_fn, [370, 400], 0.01, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1113279c-f697-4735-af0a-a74ed1a7c7dc",
   "metadata": {},
   "source": [
    "So we notice here that the results greatly depend on parameters fed into the algorithm. Which sort of implies, that this is a guessing game if the nature of the objective function is not already known. Is there some way we can optimize the algorithm to limit the amount of parameters needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254ac915-2b95-44cc-af4c-a2afb950953f",
   "metadata": {},
   "source": [
    "Let's try setting a terminal/stopping condition instead of a manual input of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf294530-54aa-49ad-834e-01745c5454ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_terminating_line_search(objective_function, initial_point, step_size):\n",
    "    '''\n",
    "    returns arguments which identifies local minima of given objective function given \n",
    "        initial points, \n",
    "        step size\n",
    "    '''\n",
    "    \n",
    "    x0 = np.array([float(i) for i in initial_point])\n",
    "    stop = False\n",
    "    \n",
    "    while stop == False:\n",
    "        \n",
    "        gradient = np.array(grad(objective_function)(x0))*-1 # descent direction determined by negative gradient\n",
    "        unit_direction = gradient/np.linalg.norm(gradient) # desecnt direction factor effect minized to unit vector\n",
    "        x = list(x0 + unit_direction*step_size) # x is updated with x + step_size*direction\n",
    "        \n",
    "        if objective_function(x0) <= objective_function(x):\n",
    "            stop = True #stop \"searching\" if the results of the next input values is bigger or equal to the previous input value\n",
    "        else:\n",
    "            x0 = x\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "703af921-367c-46a1-8cc8-116c4dc9a491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.64 s, sys: 29.3 ms, total: 1.67 s\n",
      "Wall time: 1.62 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.005183366570926999, -0.007004549420172505]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "self_terminating_line_search(obj_fn, [37, 50], 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c24cc79d-db87-438c-a198-eed3a5795b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 609 ms, sys: 54 ms, total: 663 ms\n",
      "Wall time: 587 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[25.103139891794168, 33.92316201594099]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "naive_line_search(obj_fn, [37, 50], 0.01, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fff20c-6391-4efe-bc3b-1f14be84904a",
   "metadata": {},
   "source": [
    "We see that although we get a better result and have 1 less parameter we need to \"guess\", if we start off with not so good parameters, the algorithm can take forever to run. Case and point if I were to change the step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4de99e0-49db-4faa-ade5-9c695b314284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 435 ms, sys: 879 µs, total: 436 ms\n",
      "Wall time: 379 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.028977086790774598, -0.03915822539293959]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "self_terminating_line_search(obj_fn, [37, 50], 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f898b4-e97c-456c-ae49-03bf4233c6d9",
   "metadata": {},
   "source": [
    "So in this case, is there a way we can improve (perhaps optimize) our selection of step size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9468306-1f78-45ca-ae86-c3e2232557fe",
   "metadata": {},
   "source": [
    "### Armijo + Wolfe Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20601db9-5b46-4f90-b6b5-845f4533e867",
   "metadata": {},
   "source": [
    "The Armijo and subsequently Wolfe conditions allows for the descent algorithm to select a better suited step length for every iteration (after the initialization) with given parameters (Armijo: c1, Wolfe: c1, c2, all 0 < c < 1 for all c). \n",
    "\n",
    "Below is the Armijo condition function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f61a5ff-fc9f-484a-9b1c-ce1be2192b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def armijo(objective_function, x_current, search_direction, c1, initial_stepsize):\n",
    "    \n",
    "    stepsize = initial_stepsize\n",
    "    x_next = x_current + stepsize*search_direction\n",
    "    f_current = objective_function(x_current)\n",
    "    f_next = objective_function(x_next)\n",
    "    gradient_current = np.array(grad(objective_function)(x_current))\n",
    "    \n",
    "    while ((f_next) > (f_current + c1 * stepsize * np.dot(gradient_current, search_direction))):\n",
    "        stepsize = stepsize*0.5\n",
    "        x_next = x_current + stepsize * search_direction\n",
    "        f_next = objective_function(x_next)\n",
    "        \n",
    "    return stepsize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a7aa76-384f-4a78-86c4-0cfa2898140a",
   "metadata": {},
   "source": [
    "As you can see, it is meant to take each input iteration, and return the ideal step_size to continue next descent iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f683a979-8fca-4e50-af80-5f445889ac06",
   "metadata": {},
   "source": [
    "Below is the Wolfe condition function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8eee4573-0554-4d20-acfc-309cbf79fc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wolfe(objective_function, x_current, search_direction, c1, c2, initial_stepsize):\n",
    "    \n",
    "    stepsize = initial_stepsize\n",
    "    x_next = x_current + stepsize*search_direction\n",
    "    f_current = objective_function(x_current)\n",
    "    f_next = objective_function(x_next)\n",
    "    gradient_current = np.array(grad(objective_function)(x_current))\n",
    "    gradient_next = np.array(grad(objective_function)(x_next))\n",
    "    \n",
    "    lower_bound = 0\n",
    "    upper_bound = np.inf\n",
    "    \n",
    "    while True:\n",
    "        if ((f_next) > (f_current + c1 * stepsize * np.dot(gradient_current, search_direction))):\n",
    "            upper_bound = stepsize\n",
    "            stepsize = (lower_bound + upper_bound)*0.5\n",
    "        elif (np.dot(gradient_next, search_direction) < c2 * np.dot(gradient_current, search_direction)):\n",
    "            lower_bound = stepsize\n",
    "            if np.isinf(upper_bound):\n",
    "                stepsize = lower_bound*2.0\n",
    "            else:\n",
    "                stepsize = (lower_bound + upper_bound)*0.5\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        x_next = x_current + stepsize * search_direction\n",
    "        f_next = objective_function(x_next)\n",
    "        gradient_next = np.array(grad(objective_function)(x_next))\n",
    "    \n",
    "    return stepsize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f84be36-a90f-49cb-bae0-d35de0193d5c",
   "metadata": {},
   "source": [
    "Let's incorporate these into descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "470271b8-adf7-48c3-9ae2-256994d8316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aw_line_search(objective_function, initial_point, c1, c2, initial_stepsize, condition='armijo'):\n",
    "    x0 = np.array([float(i) for i in initial_point])\n",
    "    stop = False\n",
    "    \n",
    "    while stop == False:\n",
    "        gradient = np.array(grad(objective_function)(x0))*-1 # descent direction determined by negative gradient\n",
    "        unit_direction = gradient/np.linalg.norm(gradient) # desecnt direction factor effect minized to unit vector\n",
    "        \n",
    "        # find stepsize to calculate next x\n",
    "        if condition.lower() == 'armijo':\n",
    "            stepsize = armijo(objective_function, x0, unit_direction, c1, initial_stepsize)\n",
    "        elif condition.lower() == 'wolfe':\n",
    "            stepsize = wolfe(objective_function, x0, unit_direction, c1, c2, initial_stepsize)\n",
    "        else:\n",
    "            print('condition name required')\n",
    "            break\n",
    "        \n",
    "        x = list(x0 + stepsize * unit_direction)\n",
    "        \n",
    "        if objective_function(x0) <= objective_function(x):\n",
    "            stop = True #stop \"searching\" if the results of the next input values is bigger or equal to the previous input value\n",
    "        else:\n",
    "            x0 = x\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72b53b79-6dab-434b-8e6c-e07d0f82a12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.66 s, sys: 57.7 ms, total: 3.72 s\n",
      "Wall time: 3.66 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.346248810556834e-163, 7.224660554807402e-163]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "aw_line_search(obj_fn, [37, 50], 0.1, 0.75, 0.01, condition='armijo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0186009e-1b3b-4f29-8f8c-6d08ad5cb49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.4 s, sys: 11.5 ms, total: 13.4 s\n",
      "Wall time: 13.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.060513460866673e-163, 1.0892585757926226e-162]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "aw_line_search(obj_fn, [37, 50], 0.1, 0.75, 0.01, condition='wolfe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82346a87-2ff5-4258-974f-9117bc70bd14",
   "metadata": {},
   "source": [
    "As we see here, both the use of Armijo and Wolfe conditions yielded superior results comparing to the naive cases earlier.\n",
    "However, it was done so with the expense of more time (another loop to search for ideal stepsize for every descent iterations) and it doesn't really relieve the issues of guessing parameters (in this case hyperparameters c1 and c2, depending on condition). So there is some sort of trade off here, if precision is much more important, then the usage of the conditions are recommended, if not then naive approach maybe a better idea. This is potentially why hyperparameter tuning of machine learning model training are on the more naive end of the spectrum in practice.\n",
    "\n",
    "Furthermore, if the wrong hyperparameters (c1 and c2) are used, the descent path can deteriorate leading to an invalid solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dd30850-ee32-4d95-8edd-176d3755fced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.85 s, sys: 39.7 ms, total: 3.89 s\n",
      "Wall time: 3.87 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-adea20f55111>:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  unit_direction = gradient/np.linalg.norm(gradient) # desecnt direction factor effect minized to unit vector\n",
      "<ipython-input-14-05f488395449>:9: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  while ((f_next) > (f_current + c1 * stepsize * np.dot(gradient_current, search_direction))):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-inf, -inf]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "aw_line_search(obj_fn, [37, 50], 0.01, 0.75, 0.01, condition='armijo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c51e675c-2edd-4b2f-a15a-7a7b610c53fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-adea20f55111>:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  unit_direction = gradient/np.linalg.norm(gradient) # desecnt direction factor effect minized to unit vector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.8 s, sys: 0 ns, total: 16.8 s\n",
      "Wall time: 16.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-d52f5df1e5bd>:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if ((f_next) > (f_current + c1 * stepsize * np.dot(gradient_current, search_direction))):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[inf, inf]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "aw_line_search(obj_fn, [37, 50], 0.01, 0.75, 0.01, condition='wolfe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7d3c1c-aaf1-47bc-b51a-9259e90303b4",
   "metadata": {},
   "source": [
    "### Newton Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c4c492-ea88-461f-a906-888a347e063b",
   "metadata": {
    "tags": []
   },
   "source": [
    "If the 2nd derivative (or hessian) can be derived from the objective function, then the descent direction can be computed using both the hessian and gradient. The idea here is to use Newton method of root finding to identify roots of the gradient (i.e. set of inputs which have gradient = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27be10c8-2788-408f-8b10-993b89d40e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import hessian\n",
    "from numpy.linalg import inv\n",
    "\n",
    "def newton_method(objective_function, initial_point, stepsize, iterations):\n",
    "    i = 0\n",
    "    x = np.array([float(i) for i in initial_point])\n",
    "    \n",
    "    while i < iterations:\n",
    "        gradient = np.array(grad(objective_function)(x)) \n",
    "        h = hessian(objective_function)(np.array(x))\n",
    "        \n",
    "        direction = -1.0*np.dot(inv(h),gradient) \n",
    "        \n",
    "        x = list(x + direction*stepsize) # x is updated with x + step_size*direction\n",
    "        i += 1\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54642822-eb2a-48b0-81a7-2d9be4068206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.66 s, sys: 0 ns, total: 2.66 s\n",
      "Wall time: 2.65 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "newton_method(obj_fn, [37, 50], 1, 2000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db8902f4-d93a-44d4-a819-6e5534b0294d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.64 s, sys: 18 ms, total: 2.65 s\n",
      "Wall time: 2.63 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1303699476614377e-90, 1.5275269562992385e-90]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "newton_method(obj_fn, [37, 50], 0.1, 2000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5425dc5-c92d-4f29-ba16-21bdf50610a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.22 s, sys: 40.2 ms, total: 2.26 s\n",
      "Wall time: 2.25 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.895899431071434e-08, 9.318783014961302e-08]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "newton_method(obj_fn, [37, 50], 0.01, 2000) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9faf20-9c41-4197-a894-c1784c005cff",
   "metadata": {},
   "source": [
    "In the event where 2nd order derivative is not computable, a \"secant\" method can be used where the 2nd order derivatives are approximated by the secants of 1st order derivates. I will not dive down this rabbit hole for the time being."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84abf56-aba8-4b96-b2ef-108558a235f4",
   "metadata": {},
   "source": [
    "## Trust Region Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f41d57-3fe3-48aa-bbcf-eb9fdff18314",
   "metadata": {},
   "source": [
    "What if a skeptic comes along and say algorithms shouldn't place too much trust on first-order and second-order conditions? What if a large magnitude of gradient results in overstepping of the descent? (i.e. skipping over the local minima). Trust region methods are employed where the descent iterations are more restricted (preventing overstepping). It predicts both the movemnt associated with the step and also limits the step taken.\n",
    "\n",
    "Trust region methods choose to determine step size prior to determining step direction unlike line search methods. The name trust region comes from determining the next step by minimizing the objective function around an area of the current iteration. \n",
    "\n",
    "Trust region methods also examines on whether the resulting prediction increment is \"good enough\" to determine whether there should be an increase in radius to consider for next increment estimation.\n",
    "\n",
    "Below is a sample of a trust region descent method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a59bb951-624c-48c3-affa-1cb97d5e0d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_trust_region_subproblem(objective_function, gradient_x, hessian_x, x, delta, lagrange_multiplier_init = 1):\n",
    "    '''\n",
    "    minimizes function m(p) = f(k) + f'(k)*p + 1/2*f''(k)*p^2\n",
    "    such that norm(p) <= delta\n",
    "    \n",
    "    p will be the step length of the next step\n",
    "    '''\n",
    "    \n",
    "    # p_prime = solved optimization problem\n",
    "    \n",
    "    x_prime = x + p_prime\n",
    "    y_prime = obj_fn(x) + np.dot(gradient_x, p_prime) + 0.5*np.matmul(np.matmul(np.transpose(p_prime), hessian_x), p_prime) \n",
    "    \n",
    "    return x_prime, y_prime\n",
    "\n",
    "def trust_region_descent(objective_function, x, k_max, eta_1, eta_2, gamma_1, gamma_2, delta):\n",
    "    '''\n",
    "    eta_1 = threshhold and how much to shrink radius with bad prediction\n",
    "    eta_2 = threshhold and how much to increase  radius with good prediction\n",
    "    delta = initial region radius\n",
    "    '''\n",
    "    \n",
    "    y = objective_function(x)\n",
    "    g = np.array(grad(objective_function)(x))\n",
    "    h = np.array(hessian(objective_function)(x))\n",
    "    \n",
    "    for k in range(0, k_max):\n",
    "        x_prime, y_prime = solve_trust_region_subproblem(objective_function, g, h, x, delta)\n",
    "        \n",
    "        print(x_prime, y_prime)\n",
    "        trust_region_radius_ratio = (y - objective_function(x_prime))/(y - y_prime)\n",
    "        \n",
    "        if trust_region_radius_ratio < eta_1:\n",
    "            delta = gamma_1*delta # shrinks next radius of region if actual drop is worse than expected\n",
    "        else:\n",
    "            x = x_prime # updates x if actual drop is better than expected\n",
    "            \n",
    "            y = objective_function(x)\n",
    "            g = np.array(grad(objective_function)(x))\n",
    "            h = np.array(hessian(objective_function)(x))\n",
    "            \n",
    "            if trust_region_radius_ratio > eta_2:\n",
    "                delta = gamma_2*delta # increase next radius of region if actual drop is better than expected\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c431643-3650-4e9d-a912-da7975794352",
   "metadata": {},
   "source": [
    "Note that the subproblem in trust region is a constrained optimization problem with inequality constraints. Therefore, the code is only used to illustrate the intuition of the algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
